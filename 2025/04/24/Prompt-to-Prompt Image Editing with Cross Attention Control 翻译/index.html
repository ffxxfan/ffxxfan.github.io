<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Prompt-to-Prompt Image Editing with Cross Attention Control | 华生</title><meta name="author" content="华生,ffxxfan@gmail.com"><meta name="copyright" content="华生"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文翻译">
<meta property="og:type" content="article">
<meta property="og:title" content="Prompt-to-Prompt Image Editing with Cross Attention Control">
<meta property="og:url" content="https://ffxxfan.github.io/2025/04/24/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attention%20Control%20%E7%BF%BB%E8%AF%91/index.html">
<meta property="og:site_name" content="华生">
<meta property="og:description" content="论文翻译">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424002826556.png">
<meta property="article:published_time" content="2025-04-23T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-23T16:00:00.000Z">
<meta property="article:author" content="华生">
<meta property="article:tag" content="论文翻译">
<meta property="article:tag" content="图像编辑">
<meta property="article:tag" content="图像生成">
<meta property="article:tag" content="prompt">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424002826556.png"><link rel="shortcut icon" href="/img/avatar.png"><link rel="canonical" href="https://ffxxfan.github.io/2025/04/24/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attention%20Control%20%E7%BF%BB%E8%AF%91/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Prompt-to-Prompt Image Editing with Cross Attention Control',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-04-24 00:00:00'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 学习</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/fundamentals/"><i class="fa-fw fas fa-desktop"></i><span> 基础知识</span></a></li><li><a class="site-page child" href="/terminology/"><i class="fa-fw fas fa-bookmark"></i><span> 术语规范</span></a></li><li><a class="site-page child" href="/paper/"><i class="fa-fw fas fa-file-alt"></i><span> 论文</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/passian/"><i class="fa-fw fas fa-lightbulb"></i><span> 安利</span></a></li><li><a class="site-page child" href="/advocate/"><i class="fa-fw fas fa-star"></i><span> 杂谈</span></a></li><li><a class="site-page child" href="/bucketList/"><i class="fa-fw fas fa-bullseye"></i><span> 人生清单</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-caret-down"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/board/"><i class="fa-fw fas fa-book"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><!-- 添加调试信息 huasheng 20240919--><!-- - console.log('Debug: page =', page)--><!-- - console.log('Debug: page.posts =', page.posts.type)--><!-- - if (page.posts && page.posts.length > 0)--><!--   - page.posts.forEach(post => {console.log('Post type:', post.type)})--><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424002826556.png')"><nav id="nav"><span id="blog-info"><a href="/" title="华生"><span class="site-name">华生</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 学习</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/fundamentals/"><i class="fa-fw fas fa-desktop"></i><span> 基础知识</span></a></li><li><a class="site-page child" href="/terminology/"><i class="fa-fw fas fa-bookmark"></i><span> 术语规范</span></a></li><li><a class="site-page child" href="/paper/"><i class="fa-fw fas fa-file-alt"></i><span> 论文</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/passian/"><i class="fa-fw fas fa-lightbulb"></i><span> 安利</span></a></li><li><a class="site-page child" href="/advocate/"><i class="fa-fw fas fa-star"></i><span> 杂谈</span></a></li><li><a class="site-page child" href="/bucketList/"><i class="fa-fw fas fa-bullseye"></i><span> 人生清单</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-caret-down"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/board/"><i class="fa-fw fas fa-book"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Prompt-to-Prompt Image Editing with Cross Attention Control</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-23T16:00:00.000Z" title="发表于 2025-04-24 00:00:00">2025-04-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-23T16:00:00.000Z" title="更新于 2025-04-24 00:00:00">2025-04-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Prompt-to-Prompt Image Editing with Cross Attention Control"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="prompt-to-prompt-image-editing-with-cross-attention-control-翻译">Prompt-to-Prompt Image Editing with Cross Attention Control 翻译</h1>
<h2 id="abstract">Abstract</h2>
<p>近期，大规模文本驱动合成模型因其卓越能力而备受关注——它们能根据给定文本提示生成高度多样化的图像。这种基于文本的合成方法对人类尤其具有吸引力，因为人类习惯于用语言描述意图。因此，将文本驱动的图像合成扩展到文本驱动的图像编辑是顺理成章的发展。</p>
<p>对这些生成模型而言，编辑具有挑战性，因为编辑技术的固有特性是保留原始图像的大部分内容。然而在基于文本的模型中，即使对文本提示进行微小修改，也常常会导致完全不同的生成结果。现有最优方法通过要求用户提供空间掩码来定位编辑区域以缓解这一问题，但这种方式会忽略掩码区域内原有的结构和内容。</p>
<p>本文中，我们探索了一种直观的 <strong>提示到提示（prompt-to-prompt）</strong> 编辑框架，其中编辑操作仅通过文本控制。为此，我们深入分析了文本条件模型，发现交叉注意力层是控制图像空间布局与提示词之间关系的关键。基于这一发现，我们提出了多种仅通过编辑文本提示来调控图像合成的应用方案，包括：通过替换词语实现局部编辑、通过添加描述实现全局编辑，甚至能精细控制词语在图像中的呈现程度。我们在多样化图像和提示上展示了实验结果，证明该方法能实现高质量合成并忠实反映编辑后的提示要求。</p>
<h2 id="introduction">1. Introduction</h2>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423234033264.png" /></p>
<blockquote>
<p>图 1：我们的方法提供多种 <strong>提示到提示（Prompt-to-Prompt）</strong> 编辑功能：用户可调节形容词对图像的影响程度（左上）、替换图中物品（右上）、为图像指定风格（左下），或对生成图像进行进一步优化（右下）。这些操作通过扩散模型的交叉注意力机制实现，无需对图像像素空间进行任何指定。</p>
</blockquote>
<p>近年来，大型语言-图像模型（LLI）如 Imagen、DALL·E 2 和 Parti 展现出惊人的生成语义与组合能力，引发了学术界和公众的空前关注。这些模型基于超大规模图文数据集训练，采用自回归和扩散模型等最先进的图像生成技术。然而，它们缺乏简单的编辑手段，难以精准控制图像中特定语义区域——即便对文本提示进行最细微的修改，也可能导致输出图像完全改变。</p>
<p>现有 LLI 方法通过要求用户显式标注待修复区域来规避此问题，仅允许在掩码区域内修改图像，同时保持原始背景不变。虽然这种方法取得了不错的效果，但掩码操作流程繁琐，阻碍了快速直观的文本驱动编辑。更关键的是，图像掩码会丢失重要结构信息，而这些信息在修复过程中被完全忽略，导致某些编辑能力（如修改特定对象纹理）无法实现。</p>
<p>本文提出了一种直观高效的 <strong>文本编辑</strong> 方法，通过 <strong>提示到提示（Prompt-to-Prompt）</strong> 操作在预训练文本条件扩散模型中实现语义级图像编辑。我们深入解析交叉注意力层，发掘其作为图像生成控制枢纽的语义潜力。具体而言，我们聚焦于连接像素与文本标记的高维张量——<strong>交叉注意力图</strong>，发现其中蕴含的丰富语义关系对生成图像具有决定性影响。</p>
<p>我们的核心思想是：通过在扩散过程中注入交叉注意力图，控制特定扩散步骤中像素与文本标记的对应关系，从而实现图像编辑。为支持多样化创意编辑，我们提出了三种通过语义界面操控注意力图的方法（见图 1）：(1) <strong>单标记替换</strong>（如 “狗” → “猫”）时固定注意力图以保持场景构图；(2) <strong>全局编辑</strong>（如风格转换）时冻结原标记注意力，仅允许新标记获取注意力权重；(3) <strong>语义强度调节</strong>，可增强或减弱特定词语在生成图像中的影响。</p>
<p>这种仅通过修改文本提示即可实现图像编辑的直观界面，我们称之为 <strong>提示到提示</strong>。该方法支持多种传统手段难以实现的编辑任务，且无需模型训练、微调、额外数据或优化过程。研究过程中，我们还发现了生成控制中编辑提示保真度与源图像保持度的权衡关系，并通过现有反转技术证实了该方法对真实图像的适用性。大量实验结果表明，我们的方法能以直观的文本交互方式，在极其多样化的图像上实现无缝编辑。</p>
<h2 id="related-work">2. Related work</h2>
<p>图像编辑是计算机图形学中最基础的任务之一，其核心是通过辅助输入（如标签、涂鸦、掩码或参考图像）对原始图像进行修改。最直观的编辑方式莫过于直接使用用户提供的文本提示。近年来，基于生成对抗网络（GANs）的文本驱动图像编辑技术取得了显著进展 —— GANs 以高质量生成著称，而 CLIP 通过数百万图文对训练获得的语义丰富联合表征，二者结合催生了革命性成果。这些开创性工作无需额外人工干预，仅凭文本即可实现高度逼真的图像操控。Bau 等人进一步展示了如何结合用户提供的掩码，将基于文本的编辑限定在特定空间区域。然而，尽管 GAN 方法在精选数据集（如人脸）上表现优异，面对大规模多样化数据时仍存在局限。</p>
<p>为获得更强生成能力，Crowson 等人采用基于多样化数据训练的 VQ-GAN 作为主干网络。其他研究则利用扩散模型实现当前最先进的生成质量，这类模型在多样化数据集上往往超越 GANs。Kim 等人实现了全局修改，而 Avrahami 等人通过用户提供的掩码成功完成局部编辑。</p>
<p>多数仅需文本输入（无需掩码）的研究仅限于全局编辑，Bar-Tal 等人提出无需掩码的文本定位编辑技术，虽在纹理修改上效果显著，但无法处理复杂结构变更（如将自行车改为汽车），且需为每个输入单独训练网络，这与我们的方法形成鲜明对比。</p>
<p>大量研究显著推进了纯文本条件图像生成（即文生图）技术的发展。近期涌现的 Imagen、DALL-E2 和 Parti 等大规模图文模型展现出前所未有的语义生成能力，但这些模型无法仅通过文本引导控制生成结果——修改图像关联提示中的单个词语常导致完全不同的输出。例如在 “狗” 前添加 “白色” 形容词往往会改变狗的形态。为此，现有方案通常要求用户提供掩码来限定修改区域。</p>
<p>与先前工作不同，我们的方法仅需文本输入，通过利用生成模型内部层的空间信息，用户仅需修改文本提示即可直观地调整局部或全局细节。</p>
<h2 id="method">3. Method</h2>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424212805650.png" /></p>
<blockquote>
<p>图 2：通过注意力注入实现内容修改。首行展示基于原始提示 “柠檬蛋糕” 生成的图像，通过注入原始图像注意力权重，成功修改为各类蛋糕。末行仅使用相同随机种子而未注入注意力权重，导致生成结构与原图完全无关。</p>
</blockquote>
<p>设 <span class="math inline">ℐ</span> 为通过文本引导扩散模型生成的图像，其生成条件为文本提示 <span class="math inline">𝒫</span> 和随机种子 <span class="math inline"><em>s</em></span>。我们的目标是仅通过修改后的提示 <span class="math inline">𝒫<sup>*</sup></span> 来编辑输入图像，最终获得编辑图像 <span class="math inline">ℐ<sup>*</sup></span>。例如，假设用户基于提示 “我的新自行车” 生成图像后，希望修改自行车颜色、材质，甚至将其替换为滑板车，同时保持原始图像的外观和结构。最直观的交互方式是直接修改文本提示：或补充描述自行车外观，或替换关键词。与现有方法不同，我们避免依赖任何用户定义的掩码来标识编辑区域。若简单固定随机种子并使用修改后的文本重新生成，如图 2 所示，将导致图像结构与构图完全改变。</p>
<p>我们核心发现是：生成图像的结构和外观不仅取决于随机种子，更关键的是扩散过程中像素与文本嵌入的交互关系。通过调控交叉注意力层中的像素-文本交互，可实现提示到提示的图像编辑。具体而言，注入原始图像 <span class="math inline">ℐ</span> 的交叉注意力图能保持初始构图与结构。3.1 章节将阐述交叉注意力的工作原理，3.2 章节则说明如何利用其进行编辑。扩散模型基础理论请参阅附录 A。</p>
<h3 id="cross-attention-in-text-conditioned-diffusion-models-文本条件扩散模型中的交叉注意力">3.1 Cross-attention in text-conditioned Diffusion Models 文本条件扩散模型中的交叉注意力</h3>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424212849438.png" /></p>
<blockquote>
<p>图 3：方法概览。上部：视觉与文本嵌入通过交叉注意力层融合，为每个文本标记生成空间注意力图。下部：利用源图像注意力图控制生成图像的空间布局与几何结构，实现仅修改文本提示的多样化编辑。替换提示词时注入源图注意力图 <span class="math inline"><em>M</em><sub><em>t</em></sub></span> 以保持空间布局；新增短语时仅注入未修改部分的注意力图；通过重加权注意力图可调节词语语义强度。</p>
</blockquote>
<p>我们采用 Imagen 文本引导合成模型作为基础架构。由于图像构图和几何结构主要在 <span class="math inline">64 × 64</span> 分辨率阶段确定，故直接沿用原始超分辨率模块。每个扩散步 <span class="math inline"><em>t</em></span> 通过 U-Net 网络从噪声图像 <span class="math inline"><em>z</em><sub><em>t</em></sub></span> 和文本嵌入 <span class="math inline"><em>ψ</em>(𝒫)</span> 预测噪声 <span class="math inline"><em>ϵ</em></span>，最终生成图像 <span class="math inline">ℐ = <em>z</em><sub>0</sub></span>。关键之处在于：噪声预测过程中，视觉与文本特征的嵌入通过交叉注意力层融合，该层会为每个文本标记生成空间注意力图。</p>
<p>如图 3 (上)所示，噪声图像的空间特征 <span class="math inline"><em>ϕ</em>(<em>z</em><sub><em>t</em></sub>)</span> 经线性投影 <span class="math inline">ℓ<sub><em>Q</em></sub></span> 得到查询矩阵 <span class="math inline"><em>Q</em></span>，文本嵌入则通过 <span class="math inline">ℓ<sub><em>K</em></sub>, ℓ<sub><em>V</em></sub></span> 投影为键矩阵 <span class="math inline"><em>K</em></span> 和值矩阵 <span class="math inline"><em>V</em></span>。注意力图计算公式为：</p>
<p><span class="math display">$$\begin{equation}
M =\text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right),
\end{equation}$$</span></p>
<p>其中 <span class="math inline"><em>M</em><sub><em>i</em><em>j</em></sub></span> 表示第 <span class="math inline"><em>j</em></span> 个文本标记对第 <span class="math inline"><em>i</em></span> 个像素的影响权重，<span class="math inline"><em>d</em></span> 为键值查询的潜在投影维度。交叉注意力输出 <span class="math inline"><em>ϕ̂</em>(<em>z</em><sub><em>t</em></sub>) = <em>M</em><em>V</em></span> 用于更新空间特征 <span class="math inline"><em>ϕ</em>(<em>z</em><sub><em>t</em></sub>)</span>。实践中采用多头注意力机制增强表达能力。</p>
<p>Imagen 与 GLIDE 类似，通过两类注意力层实现文本条件控制：i) 纯交叉注意力层；ii) 将文本嵌入序列与自注意力键值对拼接的混合注意力层。本文统称为交叉注意力，因我们仅干预其中涉及文本标记的部分。</p>
<h3 id="controlling-the-cross-attention-交叉注意力控制">3.2 Controlling the Cross-attention 交叉注意力控制</h3>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424213116967.png" /></p>
<blockquote>
<p>图 4：文本条件扩散生成的交叉注意力图。首行显示左侧合成图像各词语的平均注意力掩码，底部各行展示不同扩散步数下 “熊” 与 “鸟” 的注意力图分布。</p>
</blockquote>
<p>核心观察在于：生成图像的空间布局和几何结构取决于交叉注意力图。如图 4 所示，像素会 “关注” 描述它们的词语（如 “熊” 像素与对应词语强相关）。值得注意的是，图像结构在扩散早期阶段即已确定。</p>
<p>由于注意力图反映了整体构图，我们可以将原始提示 <span class="math inline">𝒫</span> 生成的注意力图 <span class="math inline"><em>M</em></span> 注入到修改提示 <span class="math inline">𝒫<sup>*</sup></span> 的生成过程中。这样既能根据新提示编辑图像，又能保持原始结构。这是基于注意力操控实现多样化编辑的典型案例。</p>
<p>定义 <span class="math inline"><em>D</em><em>M</em>(<em>z</em><sub><em>t</em></sub>, 𝒫, <em>t</em>, <em>s</em>)</span> 为扩散过程单步计算，输出噪声图像 <span class="math inline"><em>z</em><sub><em>t</em> − 1</sub></span> 和注意力图 <span class="math inline"><em>M</em><sub><em>t</em></sub></span>（未使用时省略）。 <span class="math inline"><em>D</em><em>M</em>(<em>z</em><sub><em>t</em></sub>, 𝒫, <em>t</em>, <em>s</em>){<em>M</em> ← <em>M̂</em>}</span> 表示用给定图 <span class="math inline"><em>M̂</em></span> 覆盖原注意力图，但保留提示对应的 <span class="math inline"><em>V</em></span> 值。<span class="math inline"><em>M</em><sub><em>t</em></sub><sup>*</sup></span> 为修改提示 <span class="math inline">𝒫<sup>*</sup></span> 生成的注意力图。通用编辑函数 <span class="math inline"><em>E</em><em>d</em><em>i</em><em>t</em>(<em>M</em><sub><em>t</em></sub>, <em>M</em><sub><em>t</em></sub><sup>*</sup>, <em>t</em>)</span> 接收原始与编辑图像在第 <span class="math inline"><em>t</em></span> 步的注意力图作为输入。</p>
<p>我们提出的可控图像生成通用算法通过同步执行双提示词的迭代扩散过程实现，其中每个扩散步骤都会根据编辑需求施加基于注意力的操控。需特别说明的是，该方法必须固定内部随机种子——由于扩散模型的特性，即使相同提示词，不同随机种子也会产生截然不同的输出。算法形式化描述如下：</p>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/image-20250424001334118.png" /></p>
<p>若将提示词 <span class="math inline">𝒫</span> 与种子 <span class="math inline"><em>s</em></span> 生成的图像 <span class="math inline">ℐ</span> 作为额外输入，算法结构保持不变（真实图像编辑参见附录）。实际上，第 7 行的前向计算可通过在扩散函数内部应用编辑函数来省略。此外，对 <span class="math inline"><em>z</em><sub><em>t</em> − 1</sub></span> 和 <span class="math inline"><em>z</em><sub><em>t</em></sub><sup>*</sup></span> 的扩散步骤可并行批处理，因此相较原始扩散推理仅增加单步开销。</p>
<p>下面具体阐述三种编辑操作对应的 <span class="math inline"><em>E</em><em>d</em><em>i</em><em>t</em>(<em>M</em><sub><em>t</em></sub>, <em>M</em><sub><em>t</em></sub><sup>*</sup>, <em>t</em>)</span> 函数实现（架构概览见图 3（下））：</p>
<p><strong>Word Swap（词语替换）</strong>。当用户替换提示词（如 <span class="math inline">𝒫=</span> “红色大自行车” → <span class="math inline">𝒫<sup>*</sup>=</span> “红色大汽车”）时，需在保持原始构图的同时适应新内容。为此我们注入源图像的注意力图，但硬约束可能导致几何结构不适配（如 “汽车” → “自行车” 这类大幅结构修改）。采用柔性注意力约束方案： <span class="math display">$$Edit(M_{t}, M_{t}^*, t) :=     \begin{cases}
      M_{t}^* &amp;\quad\text{if} \; t &lt; \tau\\
      M_{t} &amp;\quad\text{otherwise} \\
     \end{cases}$$</span> 其中时间步参数 <span class="math inline"><em>τ</em></span> 控制注入持续步数。由于构图在扩散早期即确定，限制注入步数可在保持大体构图的同时，为新提示保留必要的几何调整自由度（示例见附录）。对于多 token 词语，可通过下一段描述的对齐函数进行复制/平均处理。</p>
<p><strong>Adding a New Phrase（新增短语）</strong>。当用户扩展提示词（如 <span class="math inline">𝒫=</span> “河畔城堡” → <span class="math inline">𝒫<sup>*</sup>=</span> “儿童绘制的河畔城堡”）时，仅对共有 token 注入注意力以保留共有细节。定义对齐函数 <span class="math inline"><em>A</em></span> 将目标提示词 token 索引映射至源提示词对应索引（无匹配则返回 None），则编辑函数为： <span class="math display">$$\left(Edit\left(M_{t}, M_{t}^*, t\right)\right)_{i, j}:=     \begin{cases}
      (M_{t}^*)_{i, j} &amp;\quad\text{if} \; A(j) = None\\
      (M_{t})_{i, A(j)} &amp;\quad\text{otherwise} \\
     \end{cases}$$</span> 其中 <span class="math inline"><em>i</em></span> 为像素索引，<span class="math inline"><em>j</em></span> 为文本 token 索引。同样可通过 <span class="math inline"><em>τ</em></span> 控制注入步数。此类编辑支持风格化、对象属性指定等全局操控（示例见附录）。</p>
<p><strong>Attention Re–weighting（注意力重加权）</strong>。当用户需要调节特定 token 的影响强度（如 <span class="math inline">𝒫=</span> “蓬松红球” 中 “蓬松” 程度）时，对指定 token <span class="math inline"><em>j</em><sup>*</sup></span> 的注意力图施加系数 <span class="math inline"><em>c</em> ∈ [ − 2, 2]</span> 进行缩放： <span class="math display">$$\left(Edit\left(M_{t}, M_{t}^*, t\right)\right)_{i, j}:=     \begin{cases}
     c \cdot (M_{t})_{i, j} &amp;\quad\text{if }j = j^*\\
       (M_{t})_{i, j} &amp;\quad\text{otherwise} \\
     \end{cases}$$</span> 如附录所示，系数 <span class="math inline"><em>c</em></span> 能实现精细直观的效果调控。</p>
<h2 id="applications">4. Applications</h2>
<p>如第 3 节所述，我们通过控制用户提示词中各词语对应的空间布局，实现了仅需文本的直观编辑。本节将展示该技术的多种应用场景。</p>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424213033136.png" /></p>
<blockquote>
<p>图 5：对象保持。仅注入左上图 “蝴蝶” 一词的注意力权重，可在替换背景时保持该对象结构与外观。注意蝴蝶以合理姿态停留在所有物体上方。</p>
</blockquote>
<p><strong>Text-Only Localized Editing（纯文本定位编辑）</strong>。我们首先演示无需用户提供掩码的定位编辑。图 2 中，使用提示词 “柠檬蛋糕” 生成图像后，将 “柠檬” 替换为 “南瓜” 时（首行），本方法能完美保持空间布局、几何结构与语义关系——左上角的柠檬同步变为南瓜，而背景细节完整保留。相比之下，直接输入 “南瓜蛋糕” 提示词（第三行）即使采用确定性 DDIM 框架并使用相同随机种子，仍会导致完全不同的几何结构。本方法甚至能处理 “意大利面蛋糕” 这类挑战性提示（第二行），成功生成覆盖番茄酱的千层面蛋糕。图 5 展示了另一种模式：仅注入 “蝴蝶” 一词的注意力图，在改变其他内容时保持蝴蝶原貌（更多示例见附录图 13）。</p>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424213101209.png" /></p>
<blockquote>
<p>图 6：不同扩散步数的注意力注入效果。顶部为源图像与提示词，各行通过替换单个词语并注入 0%（左）至 100%（右）步数的注意力图实现内容修改。未使用本方法时（左）无法保证保留任何源图内容，而全程注入（右）可能过度约束几何结构导致文本保真度下降（如第三行汽车变为自行车）。</p>
</blockquote>
<p>如图 6 所示，本方法不仅限于纹理修改，还能实现 “自行车” 变 “汽车” 等结构改造。左栏展示未注入注意力时的结果——单词语替换导致完全不同的输出。从左至右依次显示增加注意力注入步数后的生成效果：注入步数越多，对原始图像的保真度越高。但最佳效果未必需要全程注入，因此通过调整注入步数，用户可精准控制保真度。</p>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424213019381.png" /></p>
<blockquote>
<p>图 7：提示词细化编辑。通过扩展初始提示描述，可实现汽车局部编辑（上）或全局修改（下）。</p>
</blockquote>
<p>除词语替换外，用户也可为图像添加新描述。此时我们保留原始提示的注意力图，仅对新词进行生成。例如图 7（上）中，为 “汽车” 添加 " crushed" 描述后，背景完好保留的同时车辆新增破碎细节（更多示例见附录图 14）。</p>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424214115275.png" /></p>
<blockquote>
<p>图 8：图像风格化。在注入源图注意力图的同时添加风格描述，可生成保持原图结构的新风格图像。</p>
</blockquote>
<p><strong>Global editing（全局编辑）</strong>。保持图像构图不仅对定位编辑有价值，也是全局编辑的重要维度。此类编辑需影响整个图像，同时保留对象位置与特征等核心构图。如图 7（下）所示，添加 “snow” 或改变光照条件时，图像内容仍保持连贯。图 8 还展示了将草图转为照片级图像、施加艺术风格等应用。</p>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424213053161.png" /></p>
<blockquote>
<p>图 9：带调节器的文本编辑。通过减小（上）或增大（下）指定词语（箭头标记）的交叉注意力权重，精确控制其对生成图像的影响程度。</p>
</blockquote>
<p><strong>Fader Control using Attention Re-weighting（注意力重加权调节器）</strong>。虽然通过修改提示词控制图像效果显著，但我们发现其对生成结果的控制仍不够精细。以 “snowy mountain” 为例，用户可能需要调节积雪量，但用文字精确描述期望程度十分困难。为此我们提出类似图 9 的调节器控制方案，通过重缩放特定词语的注意力强度来实现量化控制（实现原理见第 3 节，更多结果见附录图 15）。</p>
<p><strong>Real Image Editing（真实图像编辑）</strong>。编辑真实图像需要寻找能通过扩散过程重建原始图像的初始噪声向量（即反转过程）。虽然该技术在 GAN 领域已有深入研究，但在文本引导扩散模型中尚未完全解决。</p>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424214051545.png" /></p>
<blockquote>
<p>图 10：真实图像编辑。左列采用 DDIM 采样反转结果，通过从真实图像反向扩散获得近似潜在噪声；右列应用 Prompt-to-Prompt 技术进行编辑。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424214127553.png" /></p>
<blockquote>
<p>图 11：反转失败案例。当前基于 DDIM 的真实图像反转可能产生不理想的重建效果。</p>
</blockquote>
<p>我们基于扩散模型常见反转技术展示了初步编辑效果。基础方案是向输入图像添加高斯噪声后执行固定步数扩散，但会产生明显失真。改进方案采用 DDIM 确定性模型的反向扩散过程（<span class="math inline"><em>x</em><sub>0</sub> → <em>x</em><sub><em>T</em></sub></span>），设定 <span class="math inline"><em>x</em><sub>0</sub></span> 为真实图像。如图 10 所示，该方案常能获得满意结果，但图 11 等案例显示其反转精度仍不足——这源于失真与可编辑性的权衡：降低分类器无关引导参数虽能改善重建质量，却会限制大幅编辑能力。</p>
<p>为缓解该限制，我们提出基于注意力图自动生成掩码来修复未编辑区域。如图 12 所示，即使采用基础 DDIM 反转方案（加噪后去噪），该方法在各种编辑操作下都能良好保持猫咪特征，而掩码完全由提示词自动产生。</p>
<h2 id="conclusions">5. Conclusions</h2>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424214100589.png" /></p>
<blockquote>
<p>图 12：基于掩码的编辑。当反转失真显著时，利用注意力图自动生成掩码保护未编辑区域（无需用户提供掩码）。编辑后猫咪特征保持完好。</p>
</blockquote>
<p>本研究揭示了文生图扩散模型中交叉注意力层的强大能力。我们发现这些高维层具有可解释的空间映射表征，在关联文本提示词与合成图像空间布局中起关键作用。基于此发现，我们展示了如何通过对提示词的不同操作直接控制合成图像属性，为局部和全局编辑等应用铺平道路。这项工作首次利用文本语义能力为用户提供了简单直观的图像编辑工具，使用户能在语义文本空间中逐步调整图像，而非每次修改提示词都需从头生成。</p>
<p>尽管我们已实现仅修改文本提示的语义控制，该技术仍存在若干待改进的局限：首先，当前反转过程会导致部分测试图像出现明显失真；其次，反转需要用户提供合适提示词，这对复杂构图具有挑战性（需说明文本引导扩散模型的反转问题与本研究正交，将另作深入探讨）；再次，由于交叉注意力位于网络瓶颈层，当前注意力图分辨率较低，限制了更精细的局部编辑能力，建议在未来工作中将交叉注意力引入更高分辨率层；最后，现有方法尚无法实现图像内对象的空间位置调整，这类控制也将留待后续研究。</p>
<h2 id="a-background">A Background</h2>
<h3 id="a.1-扩散模型">A.1 扩散模型</h3>
<p>去噪扩散概率模型(DDPM)是一类生成式隐变量模型，旨在建立近似数据分布 <span class="math inline"><em>q</em>(<em>x</em><sub>0</sub>)</span> 且易于采样的概率分布 <span class="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub>0</sub>)</span>。该模型定义了从数据到噪声的 “前向过程”，这是一个从 <span class="math inline"><em>x</em><sub>0</sub></span> 出发的马尔可夫链，通过逐步添加噪声生成隐变量序列 <span class="math inline"><em>x</em><sub>1</sub>, …, <em>x</em><sub><em>T</em></sub> ∈ <em>X</em></span>，满足 <span class="math inline">$q(x_1,\ldots,x_t\mid x_0)=\prod_{i=1}^{t}q(x_t\mid x_{t-1})$</span>。其中单步转移定义为高斯过程 <span class="math inline">$q(x_t\mid x_{t-1}):=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I)$</span>，由调度参数 <span class="math inline"><em>β</em><sub>0</sub>, …, <em>β</em><sub><em>T</em></sub> ∈ (0, 1)</span> 控制。当 <span class="math inline"><em>T</em></span> 足够大时，最终噪声 <span class="math inline"><em>x</em><sub><em>T</em></sub></span> 近似服从各向同性高斯分布。</p>
<p>前向过程的重要性质是可直接表达隐变量： <span class="math display">$$\begin{equation}
  x_t = \sqrt{\alpha_t}x_0+\sqrt{1-\alpha_t}w,~~w\sim N(0, I)
\end{equation} $$</span> 其中 <span class="math inline">$\alpha_t:=\prod_{i=1}^{t}(1-\beta_i)$</span>。</p>
<p>为从 <span class="math inline"><em>q</em>(<em>x</em><sub>0</sub>)</span> 采样，我们定义从噪声 <span class="math inline"><em>x</em><sub><em>T</em></sub></span> 到数据的 “逆向过程” <span class="math inline"><em>p</em>(<em>x</em><sub><em>t</em> − 1</sub> ∣ <em>x</em><sub><em>t</em></sub>)</span>。由于真实逆向过程 <span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub> ∣ <em>x</em><sub><em>t</em></sub>)</span> 不可解，我们使用参数化高斯转移网络 <span class="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub> ∣ <em>x</em><sub><em>t</em></sub>) := <em>N</em>(<em>x</em><sub><em>t</em> − 1</sub> ∣ <em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>), <em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>))</span> 进行近似。其中均值 <span class="math inline"><em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span> 可通过预测噪声 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span> 来计算： <span class="math display">$$\begin{equation}
\mu_\theta(x_t, t)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\alpha_t}}\epsilon_\theta(x_t, t)\right).
\end{equation}$$</span></p>
<p>训练目标为最小化： <span class="math display">min<sub><em>θ</em></sub><em>L</em>(<em>θ</em>) := min<sub><em>θ</em></sub><em>E</em><sub><em>x</em><sub>0</sub> ∼ <em>q</em>(<em>x</em><sub>0</sub>), <em>w</em> ∼ <em>N</em>(0, <em>I</em>), <em>t</em></sub>∥<em>w</em> − <em>ϵ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)∥<sub>2</sub><sup>2</sup>,</span> 通过变分下界最大化使参数 <span class="math inline"><em>θ</em></span> 拟合数据分布。</p>
<h3 id="a.2-cross-attention-in-imagen">A.2 Cross-attention in Imagen</h3>
<p>Imagen 包含三个文本条件扩散模型： 1. <strong>64×64 基础模型</strong>：从随机噪声出发，采用 U-Net 架构，在 [16,8] 分辨率层使用交叉注意力，在 [32,16,8] 层使用混合注意力进行文本条件控制</p>
<ol start="2" type="1">
<li><p><strong>64→256 超分模型</strong>：以双线性上采样图像为条件，在 32 分辨率瓶颈层使用混合注意力</p></li>
<li><p><strong>256→1024 超分模型</strong>：以双线性上采样图像为条件，仅在 64 分辨率瓶颈层使用交叉注意力</p></li>
</ol>
<h2 id="b-additional-results">B Additional results</h2>
<p>我们提供更多编辑示例： - 图 13 展示词语替换效果； - 图 14 展示描述细化编辑； - 图 15 展示注意力重加权控制；</p>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424212937042.png" /></p>
<blockquote>
<p>图 13：词语替换式 Prompt-to-Prompt 编辑的补充结果</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424212959436.png" /></p>
<blockquote>
<p>图 14：添加描述的 Prompt-to-Prompt 编辑补充结果</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424212947715.png" /></p>
<blockquote>
<p>图 15：注意力重加权式 Prompt-to-Prompt 编辑补充结果</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://ffxxfan.github.io/">华生</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://ffxxfan.github.io/2025/04/24/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attention%20Control%20%E7%BF%BB%E8%AF%91/">https://ffxxfan.github.io/2025/04/24/Prompt-to-Prompt Image Editing with Cross Attention Control 翻译/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ffxxfan.github.io" target="_blank">华生</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a><a class="post-meta__tags" href="/tags/%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/">图像编辑</a><a class="post-meta__tags" href="/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a><a class="post-meta__tags" href="/tags/prompt/">prompt</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424002826556.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/04/24/Bug%20%E5%90%91%20-%20Hexo%20-%20MathJax%20%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%B8%B2%E6%9F%93%E9%97%AE%E9%A2%98/" title="Bug 向 - Hexo - MathJax 数学公式渲染问题"><img class="cover" src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424220734049.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Bug 向 - Hexo - MathJax 数学公式渲染问题</div></div></a></div><div class="next-post pull-right"><a href="/2025/04/23/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%A0%87%E6%B3%A8%E6%96%B9%E6%B3%95/" title="文本数据标注工具"><img class="cover" src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423170608150.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文本数据标注工具</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/09/18/DynIBaR%20Neural%20Dynamic%20Image-Based%20Rendering%20%E7%BF%BB%E8%AF%91/" title="DynIBaR Neural Dynamic Image-Based Rendering 中文翻译"><img class="cover" src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423153723711.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-18</div><div class="title">DynIBaR Neural Dynamic Image-Based Rendering 中文翻译</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#prompt-to-prompt-image-editing-with-cross-attention-control-%E7%BF%BB%E8%AF%91"><span class="toc-text">Prompt-to-Prompt Image Editing with Cross Attention Control 翻译</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related-work"><span class="toc-text">2. Related work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method"><span class="toc-text">3. Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cross-attention-in-text-conditioned-diffusion-models-%E6%96%87%E6%9C%AC%E6%9D%A1%E4%BB%B6%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">3.1 Cross-attention in text-conditioned Diffusion Models 文本条件扩散模型中的交叉注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#controlling-the-cross-attention-%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8E%A7%E5%88%B6"><span class="toc-text">3.2 Controlling the Cross-attention 交叉注意力控制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#applications"><span class="toc-text">4. Applications</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusions"><span class="toc-text">5. Conclusions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#a-background"><span class="toc-text">A Background</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a.1-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="toc-text">A.1 扩散模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a.2-cross-attention-in-imagen"><span class="toc-text">A.2 Cross-attention in Imagen</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#b-additional-results"><span class="toc-text">B Additional results</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: WHITE"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By 华生</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">一起学会闪闪发光吧！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
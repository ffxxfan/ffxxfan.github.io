<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>DynIBaR Neural Dynamic Image-Based Rendering 中文翻译 | 华生</title><meta name="author" content="华生,ffxxfan@gmail.com"><meta name="copyright" content="华生"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Neural Dynamic Image-Based Rendering 中文翻译，从单目视频中合成新视角。">
<meta property="og:type" content="article">
<meta property="og:title" content="DynIBaR Neural Dynamic Image-Based Rendering 中文翻译">
<meta property="og:url" content="https://ffxxfan.github.io/2024/09/18/DynIBaR%20Neural%20Dynamic%20Image-Based%20Rendering%20%E7%BF%BB%E8%AF%91/index.html">
<meta property="og:site_name" content="华生">
<meta property="og:description" content="Neural Dynamic Image-Based Rendering 中文翻译，从单目视频中合成新视角。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423153723711.png">
<meta property="article:published_time" content="2024-09-17T16:00:00.000Z">
<meta property="article:modified_time" content="2025-05-10T18:41:19.292Z">
<meta property="article:author" content="华生">
<meta property="article:tag" content="NeRF">
<meta property="article:tag" content="新视角合成">
<meta property="article:tag" content="单目长视频新视角合成">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423153723711.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DynIBaR Neural Dynamic Image-Based Rendering 中文翻译",
  "url": "https://ffxxfan.github.io/2024/09/18/DynIBaR%20Neural%20Dynamic%20Image-Based%20Rendering%20%E7%BF%BB%E8%AF%91/",
  "image": "https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423153723711.png",
  "datePublished": "2024-09-17T16:00:00.000Z",
  "dateModified": "2025-05-10T18:41:19.292Z",
  "author": [
    {
      "@type": "Person",
      "name": "华生",
      "url": "https://ffxxfan.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.png"><link rel="canonical" href="https://ffxxfan.github.io/2024/09/18/DynIBaR%20Neural%20Dynamic%20Image-Based%20Rendering%20%E7%BF%BB%E8%AF%91/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.3.5"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DynIBaR Neural Dynamic Image-Based Rendering 中文翻译',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-compass"></i><span> 学习</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/fundamentals/"><i class="fa-fw fas fa-desktop"></i><span> 基础知识</span></a></li><li><a class="site-page child" href="/terminology/"><i class="fa-fw fas fa-bookmark"></i><span> 术语规范</span></a></li><li><a class="site-page child" href="/paper/"><i class="fa-fw fas fa-file-alt"></i><span> 论文</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/passian/"><i class="fa-fw fas fa-lightbulb"></i><span> 安利</span></a></li><li><a class="site-page child" href="/advocate/"><i class="fa-fw fas fa-star"></i><span> 杂谈</span></a></li><li><a class="site-page child" href="/bucketList/"><i class="fa-fw fas fa-bullseye"></i><span> 人生清单</span></a></li><li><a class="site-page child" href="/detailedList/"><i class="fa-fw fas fa-tasks"></i><span> 今天做什么</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-caret-down"></i><span> 其他</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/board/"><i class="fa-fw fas fa-book"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post type-paper" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423153723711.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">华生</span></a><a class="nav-page-title" href="/"><span class="site-name">DynIBaR Neural Dynamic Image-Based Rendering 中文翻译</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-compass"></i><span> 学习</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/fundamentals/"><i class="fa-fw fas fa-desktop"></i><span> 基础知识</span></a></li><li><a class="site-page child" href="/terminology/"><i class="fa-fw fas fa-bookmark"></i><span> 术语规范</span></a></li><li><a class="site-page child" href="/paper/"><i class="fa-fw fas fa-file-alt"></i><span> 论文</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/passian/"><i class="fa-fw fas fa-lightbulb"></i><span> 安利</span></a></li><li><a class="site-page child" href="/advocate/"><i class="fa-fw fas fa-star"></i><span> 杂谈</span></a></li><li><a class="site-page child" href="/bucketList/"><i class="fa-fw fas fa-bullseye"></i><span> 人生清单</span></a></li><li><a class="site-page child" href="/detailedList/"><i class="fa-fw fas fa-tasks"></i><span> 今天做什么</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-caret-down"></i><span> 其他</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/board/"><i class="fa-fw fas fa-book"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">DynIBaR Neural Dynamic Image-Based Rendering 中文翻译</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-17T16:00:00.000Z" title="发表于 2024-09-18 00:00:00">2024-09-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-10T18:41:19.292Z" title="更新于 2025-05-11 02:41:19">2025-05-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/nerf/">nerf</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="dynibar-neural-dynamic-image-based-rendering"><strong>DynIBaR: Neural Dynamic Image-Based Rendering</strong></h1>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240914154930483.png" /></p>
<blockquote>
<p>图1. 最近的方法用于从单目视频中合成新视角，例如 HyperNeRF [50] 和 NSFF [35]，在处理长视频中复杂的相机和场景运动时，渲染高质量视角仍面临挑战。我们提出了一种新的方法来解决这些局限性，以上述应用于 6DoF 视频稳定化为例，我们将该方法和之前的方法应用于一个 30 秒的抖动视频片段，并比较了在平滑相机路径上渲染的新视角（左图）。在一个动态场景数据集上（右图）[75]，我们的方法显著提高了渲染的真实性，通过合成图像和计算与移动物体对应的像素的 LPIPS 误差（黄色数字）得以体现。请参见补充视频以获取完整结果。</p>
</blockquote>
<h2 id="abstract">Abstract</h2>
<p>我们解决了从描绘复杂动态场景的单目视频中合成新视角的问题。基于时间变化的神经辐射场（即动态 NeRFs）的最先进方法在这项任务上已经展示了令人印象深刻的结果。然而，对于包含复杂物体运动和不受控制的相机轨迹的长视频，这些方法可能会产生模糊或不准确的渲染，限制了它们在实际应用中的使用。我们提出了一种新的方法，解决了这些局限性，该方法<code>采用了体积图像渲染框架，通过以场景运动感知的方式聚合附近视角的特征来合成新视角</code>。我们的系统保留了之前方法在<strong>建模复杂场景和视角依赖效果方面的优</strong>势，同时能够<strong>从包含复杂场景动态和不受限制的相机轨迹的长视频中合成照片级真实的新视角</strong>。我们在动态场景数据集上显著改进了最先进的方法，并将我们的方法应用于具有挑战性的相机和物体运动的野外视频，在这些视频中，之前的方法无法生成高质量的渲染。</p>
<h2 id="introduction">1. Introduction</h2>
<p>计算机视觉方法现在能够以惊人的质量生成静态三维场景的自由视角渲染。那么对于动态场景，如涉及人员或宠物的场景呢？从动态场景的单目视频中合成新视角是一个更具挑战性的动态场景重建问题。得益于像 HyperNeRF [50] 和 Neural Scene Flow Fields (NSFF) [35] 这样的新的时间变化神经体积表示，最近的工作在时空中合成新视角方面取得了进展。这些方法在基于坐标的多层感知机 (MLP) 中体积编码了时空变化的场景内容。</p>
<p>然而，这些动态 NeRF 方法存在一些限制，阻碍了它们在随意的实际视频中的应用。基于局部场景流的方法，如 NSFF，难以扩展到使用不受限制的相机运动拍摄的较长输入视频：NSFF 论文仅声称在 1 秒钟的前向视频上表现良好 [35]。像 HyperNeRF 这样的构建规范模型的方法主要限制在具有控制相机路径的物体中心场景中，对于具有复杂物体运动的场景可能会失败。</p>
<p>在这项工作中，我们提出了一种新的方法，该方法可扩展到以下动态视频：<code>1）较长时间持续，2）无界场景，3）不受控制的相机轨迹，以及 4）快速且复杂的物体运动。我们的方法保留了体积场景表示的优势</code>，这些表示可以建模复杂的场景几何形状和视角依赖效果，同时在静态和动态场景内容的渲染真实度上显著优于最近的方法 [35, 50]，如图1所示。</p>
<p>我们受到最近静态场景渲染方法的启发，这些方法通过沿视极线聚合来自附近视角的局部图像特征来合成新图像 [39, 64, 70]。然而，动态场景违反了这些方法所假设的视极约束。因此，我们提出<strong><em>在场景运动调整的射线空间中聚合多视角图像特征</em></strong>，这使我们能够正确地推理时空变化的几何形状和外观。</p>
<p>在将基于聚合的方法扩展到动态场景时，我们还遇到了许多效率和鲁棒性挑战。为了高效地建模跨多个视角的场景运动，我们使用<strong><em>运动轨迹场</em></strong>来建模这种运动，这些轨迹场跨越多个帧，并用学习到的基函数表示。此外，为了实现动态场景重建中的时间一致性，我们引入了一种<strong><em>新的时间光度损失，它在运动调整的射线空间中操作</em></strong>。最后，为了提高新视角的质量，我们提出通过一种<strong><em>新的基于 IBR 的运动分割技术，在贝叶斯学习框架内将场景分解为静态和动态组件</em></strong>。</p>
<h2 id="related-work">2. Related Work</h2>
<h4 id="新视角合成"><strong>新视角合成</strong></h4>
<p>新视角合成。经典的基于图像的渲染（IBR）方法通过整合输入图像的像素信息来合成新视角 [58]，并可以根据其对显式几何的依赖性进行分类。光场或光照图渲染方法 [9, 21, 26, 32] 通过过滤和插值采样射线来生成新视角，而不使用显式的几何模型。为了处理更稀疏的输入视角，许多方法 [7, 14, 18, 23, 24, 26, 30, 52, 54, 55] 利用预先计算的代理几何，如深度图或网格来渲染新视角。</p>
<p>最近，神经表示已经展示了高质量的新视角合成 [12, 17, 38, 40, 46, 48, 59–62, 72, 81]。特别是，Neural Radiance Fields (NeRF) [46] 通过在多层感知机（MLP）中编码连续场景辐射场，达到了前所未有的真实感。在所有基于 NeRF 的方法中，IBRNet [70] 与我们的工作最为相关。IBRNet 将经典的 IBR 技术与体积渲染相结合，生成了一个通用的 IBR 模块，可以在无需每场景优化的情况下渲染高质量视角。我们的工作将这种为静态场景设计的体积 IBR 框架 [11, 64, 70] 扩展到更具挑战性的动态场景。请注意，我们的重点是为具有复杂相机和物体运动的长视频合成更高质量的新视角，而不是跨场景的泛化。</p>
<h4 id="动态场景新视角合成"><strong>动态场景新视角合成</strong></h4>
<p>动态场景视角合成。我们的工作与从 RGBD [5, 15, 25, 47, 68, 83] 或单目视频 [31, 44, 78, 80] 中进行动态场景几何重建相关。然而，基于深度或网格的表示在建模复杂几何和视角依赖效果方面存在困难。</p>
<p>大多数先前针对动态场景的新视角合成工作需要多个同步输入视频 [1, 3, 6, 27, 33, 63, 69, 76, 82]，这限制了它们在实际中的应用。一些方法 [8, 13, 22, 51, 71] 使用领域知识，如模板模型，以实现高质量结果，但这些方法仅限于特定类别 [41, 56]。最近，许多工作提出从单个相机合成动态场景的新视角。Yoon 等人 [75] 通过使用单视图深度和多视图立体获得的深度图进行显式扭曲来渲染新视角。然而，这种方法未能建模复杂的场景几何，并在遮挡区域填补真实且一致的内容。随着神经渲染技术的进步，基于 NeRF 的动态视角合成方法已展示了最先进的结果 [16, 35, 53, 66, 74]。一些方法，如 Nerfies [49] 和 HyperNeRF [50]，使用变形场将每个局部观测映射到规范场景表示。这些变形以时间 [53] 或每帧潜在代码 [49, 50, 66] 为条件，并参数化为平移 [53, 66] 或刚体运动场 [49, 50]。这些方法能够处理长视频，但主要限制在相对较小物体运动和受控相机路径的物体中心场景中。其他方法将场景表示为时间变化的 NeRFs [19, 20, 35, 67, 74]。特别地，NSFF 使用神经场景流场来捕捉野外视频中的快速和复杂的三维场景运动 [35]。然而，这种方法仅在短（1-2 秒）的前向视频中效果良好。</p>
<h2 id="dynamic-image-based-rendering">3. Dynamic Image-Based Rendering</h2>
<p>给定一个动态场景的单目视频，包含帧 <span class="math inline"><em>I</em><sub>1</sub>, <em>I</em><sub>2</sub>, …, <em>I</em><sub><em>N</em></sub></span> 和已知的相机参数 <span class="math inline"><em>P</em><sub>1</sub>, <em>P</em><sub>2</sub>, …, <em>P</em><sub><em>N</em></sub></span>，我们的<strong>目标是在视频中的任何期望时间合成一个新视角</strong>。与许多其他方法类似，我们对每个视频进行训练，首先优化模型以重建输入帧，然后使用该模型渲染新视角。</p>
<p>与最近的动态 NeRF 方法直接在 MLP 的权重中编码 3D 颜色和密度不同，我们将经典的 <code>IBR</code> 思路集成到体积渲染框架中。与<strong>显式表面相比，体积表示能够更容易地建模具有视角依赖效果的复杂场景几何</strong>。</p>
<p>接下来的部分介绍了我们的方法，包括场景运动调整的<code>多视角特征聚合</code>（第 3.1 节），以及通过<code>运动调整射线空间中的跨时间渲染来强制执行时间一致性</code>（第 3.2 节）。我们的完整系统结合了静态模型和动态模型，以在每个像素处生成颜色。通过在贝叶斯学习框架中从单独训练的<code>运动分割模块</code>中获得的分割掩模来实现准确的场景分解（第 3.3 节）。 <img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240914161254499.png" /></p>
<blockquote>
<p><strong>图 2. 通过运动调整的多视角特征聚合进行渲染。</strong> 给定时间 <span class="math inline"><em>i</em></span> 上目标射线 <span class="math inline"><em>r</em></span> 处的采样位置 <span class="math inline"><em>x</em></span>，我们估计其运动轨迹，这决定了 <span class="math inline"><em>x</em></span> 在接近时间 <span class="math inline"><em>j</em> ∈ <em>N</em>(<em>i</em>)</span> 的三维对应点，记为 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub></span>。每个变形点 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub></span> 然后被投影到其对应的源视图中。沿着投影曲线提取的图像特征 <span class="math inline"><em>f</em><sub><em>j</em></sub></span> 被聚合，并馈送到具有时间嵌入 <span class="math inline"><em>γ</em>(<em>i</em>)</span> 的射线变换网络中，生成每个样本的颜色和密度 <span class="math inline">(<em>c</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>)</span>。最终像素颜色 <span class="math inline"><em>Ĉ</em><sub><em>i</em></sub></span> 通过沿射线 <span class="math inline"><em>r</em></span> 进行体积渲染 <span class="math inline">(<em>c</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>)</span> 来合成。</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240914162139432.png" /></p>
<blockquote>
<p><strong>图 3. 通过跨时间渲染实现时间一致性。</strong> 为了在动态重建中强制实现时间一致性，我们使用来自接近时间 <span class="math inline"><em>j</em></span> 的场景模型渲染每一帧 <span class="math inline"><em>I</em><sub><em>i</em></sub></span>，这种方法称为跨时间渲染。来自图像 <span class="math inline"><em>i</em></span> 的射线 <span class="math inline"><em>r</em></span> 会使用弯曲射线 <span class="math inline"><em>r</em><sub><em>i</em> → <em>j</em></sub></span> 进行渲染，即将 <span class="math inline"><em>r</em></span> 变形到时间 <span class="math inline"><em>j</em></span>。也就是说，在计算了每个沿射线 <span class="math inline"><em>r</em></span> 的采样位置处的运动调整点 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub> = <em>x</em> + <em>Δ</em><em>x</em>, <em>i</em>(<em>j</em>)</span> 后，我们通过 MLP 查询 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub></span> 和时间 <span class="math inline"><em>j</em></span>，预测其运动轨迹 <span class="math inline"><em>Γ</em><sub><em>x</em><sub><em>i</em> → <em>j</em></sub>, <em>j</em></sub></span>，沿该轨迹我们从时间 <span class="math inline"><em>k</em> ∈ <em>N</em>(<em>j</em>)</span> 的源视图中聚合图像特征 <span class="math inline"><em>f</em><sub><em>k</em></sub></span>。沿 <span class="math inline"><em>r</em><sub><em>i</em> → <em>j</em></sub></span> 的聚合特征被馈送到具有时间嵌入 <span class="math inline"><em>γ</em>(<em>j</em>)</span> 的射线变换网络，以生成时间 <span class="math inline"><em>j</em></span> 的每个样本的颜色和密度 <span class="math inline">(<em>c</em><sub><em>j</em></sub>, <em>σ</em><sub><em>j</em></sub>)</span>。通过体积渲染 <span class="math inline">(<em>c</em><sub><em>j</em></sub>, <em>σ</em><sub><em>j</em></sub>)</span> 计算出像素颜色 <span class="math inline"><em>Ĉ</em><sub><em>j</em> → <em>i</em></sub></span>，然后与真实颜色 <span class="math inline"><em>C</em><sub><em>i</em></sub></span> 比较，以形成重建损失 <span class="math inline"><em>L</em><sub>pho</sub></span>。</p>
</blockquote>
<h3 id="motion-adjusted-feature-aggregation">3.1 Motion-adjusted feature aggregation</h3>
<p>我们通过聚合从时间上相近的源视图中提取的特征来合成新视角。为了在时间 <span class="math inline"><em>i</em></span> 渲染一张图像，我们首先确定在时间 <span class="math inline"><em>i</em></span> 的时间半径 <span class="math inline"><em>r</em></span> 帧内的源视图 <span class="math inline"><em>I</em><sub><em>j</em></sub></span>，即 <span class="math inline"><em>j</em> ∈ <em>N</em>(<em>i</em>) = [<em>i</em> − <em>r</em>, <em>i</em> + <em>r</em>]</span>。对于每个源视图，我们通过共享卷积编码器网络提取 2D 特征图 <span class="math inline"><em>F</em><sub><em>j</em></sub></span>，形成输入元组 <span class="math inline">{<em>I</em><sub><em>j</em></sub>, <em>P</em><sub><em>j</em></sub>, <em>F</em><sub><em>j</em></sub>}</span>。</p>
<p>为了预测沿目标射线 <span class="math inline"><em>r</em></span> 采样的每个点的颜色和密度，我们必须在考虑场景运动的情况下聚合源视图特征。对于静态场景，沿目标射线的点将在邻近源视图中对应的极线（epipolar line）上，因此我们可以通过沿邻近的极线采样来聚合潜在的对应点。然而，动态场景中的元素会违反极线约束，如果不考虑运动，则会导致特征聚合不一致。因此，我们执行运动调整的特征聚合，如图 3 所示。</p>
<p>为了在动态场景中确定对应关系，一个直接的想法是通过 MLP [35] 估计场景流场（scene flow field），以确定给定点在邻近时间的运动调整 3D 位置。然而，由于 MLP 的递归展开，这种策略在体积 IBR 框架中计算上不可行。</p>
<p><strong><a href="">运动轨迹场。</a></strong> 我们使用运动轨迹场来表示场景运动，这些轨迹场以学习的基函数形式描述。对于给定时间 <span class="math inline"><em>i</em></span> 沿目标射线 <span class="math inline"><em>r</em></span> 的 3D 点 <span class="math inline"><em>x</em></span>，我们通过 MLP GMT 编码其轨迹系数： <span class="math display">{<em>ϕ</em><sub><em>i</em></sub><sup><em>l</em></sup>(<em>x</em>)}<sub><em>l</em> = 1</sub><sup><em>L</em></sup> = GMT(<em>γ</em>(<em>x</em>), <em>γ</em>(<em>i</em>)),</span> 其中 <span class="math inline"><em>ϕ</em><sub><em>i</em></sub><sup><em>l</em></sup> ∈ ℝ<sup>3</sup></span> 是基系数（对于 <span class="math inline"><em>x</em></span>、<span class="math inline"><em>y</em></span> 和 <span class="math inline"><em>z</em></span> 分别使用不同的系数，具体基描述如下），$ $ 表示位置编码。我们选择 <span class="math inline"><em>L</em> = 6</span> 个基和 16 个线性递增频率用于编码 <span class="math inline"><em>γ</em></span>，基于场景运动通常是低频的假设 [80]。</p>
<p>我们还引入了一个全局可学习的运动基 <span class="math inline">{<em>h</em><sub><em>i</em></sub><sup><em>l</em></sup>}<sub><em>l</em> = 1</sub><sup><em>L</em></sup></span>，其中 <span class="math inline"><em>h</em><sub><em>i</em></sub><sup><em>l</em></sup> ∈ ℝ</span>，跨越输入视频的每个时间步 <span class="math inline"><em>i</em></span>，并与 MLP 一起优化。然后，点 <span class="math inline"><em>x</em></span> 的运动轨迹定义为： <span class="math display">$$
\Gamma_{x,i}(j) = \sum_{l=1}^L h_{j}^l \phi_{i}^l(x),
$$</span> 因此，时间 <span class="math inline"><em>j</em></span> 上点 <span class="math inline"><em>x</em></span> 与其 3D 对应点 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub></span> 之间的相对位移计算为： <span class="math display"><em>Δ</em><em>x</em>, <em>i</em>(<em>j</em>) = <em>Γ</em><sub><em>x</em>, <em>i</em></sub>(<em>j</em>) − <em>Γ</em><sub><em>x</em>, <em>i</em></sub>(<em>i</em>).</span></p>
<p>通过这种运动轨迹表示，为查询点 <span class="math inline"><em>x</em></span> 在邻近视图中找到 3D 对应点只需要一次 MLP 查询，从而在我们的体积渲染框架内实现高效的多视角特征聚合。</p>
<p>我们使用 Wang 等人 [67] 提出的 DCT 基础来初始化基 <span class="math inline">{<em>h</em><sub><em>i</em></sub><sup><em>l</em></sup>}<sub><em>l</em> = 1</sub><sup><em>L</em></sup></span>，但在优化过程中将其与其他组件一起微调，因为我们观察到固定的 DCT 基础可能无法建模广泛的现实世界运动（见图 4 的第三列）。</p>
<p>利用估计的运动轨迹进行特征聚合。 使用在时间 <span class="math inline"><em>i</em></span> 对点 <span class="math inline"><em>x</em></span> 估计的运动轨迹，我们将点 <span class="math inline"><em>x</em></span> 在时间 <span class="math inline"><em>j</em></span> 的对应 3D 点表示为 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub> = <em>x</em> + <em>Δ</em><em>x</em>, <em>i</em>(<em>j</em>)</span>。我们将每个变形后的点 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub></span> 投影到其源视图 <span class="math inline"><em>I</em><sub><em>j</em></sub></span> 中，使用相机参数 <span class="math inline"><em>P</em><sub><em>j</em></sub></span>，并在投影的 2D 像素位置提取颜色和特征向量 <span class="math inline"><em>f</em><sub><em>j</em></sub></span>。对邻近视图 <span class="math inline"><em>j</em></span> 的源特征集合进行处理，使用共享的 MLP 进行特征聚合，输出特征通过加权平均池化 [70] 聚合，生成每个射线 <span class="math inline"><em>r</em></span> 上的 3D 采样点的单一特征向量。然后，具有时间嵌入 <span class="math inline"><em>γ</em>(<em>i</em>)</span> 的射线变换网络处理沿射线的特征序列，以预测每个样本的颜色和密度 <span class="math inline">(<em>c</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>)</span>（见图 2）。</p>
<p>接着，我们使用标准的 NeRF 体积渲染 [4] 从这些颜色和密度序列中获得最终像素颜色 <span class="math inline"><em>Ĉ</em><sub><em>i</em></sub>(<em>r</em>)</span>。</p>
<h3 id="cross-time-rendering-for-temporal-consistency">3.2. Cross-time rendering for temporal consistency</h3>
<p>如果我们仅通过比较 <span class="math inline"><em>Ĉ</em><sub><em>i</em></sub></span> 和 <span class="math inline"><em>C</em><sub><em>i</em></sub></span> 来优化我们的动态场景表示，可能会导致表示过拟合到输入图像上：它可能完美地重建这些视图，但无法正确渲染新的视图。这种情况可能发生，因为该表示具有为每个时间实例重建完全不同模型的能力，而没有利用或准确重建场景运动。因此，为了恢复具有物理上合理运动的一致场景，我们需要强制场景表示的时间一致性。在这种情况下，定义时间一致性的一种方法是考虑场景运动时，两个相邻时间点 <span class="math inline"><em>i</em></span> 和 <span class="math inline"><em>j</em></span> 的场景应保持一致 [19, 35, 67]。</p>
<p>特别地，我们通过在运动调整射线空间中的跨时间渲染来强制优化表示的时间光度一致性，如图 3 所示。其思路是通过一些接近时间 <span class="math inline"><em>j</em></span> 的视图来渲染时间 <span class="math inline"><em>i</em></span> 的视图，我们称之为跨时间渲染。对于每个相邻时间 <span class="math inline"><em>j</em> ∈ <em>N</em>(<em>i</em>)</span>，我们不直接使用射线 <span class="math inline"><em>r</em></span> 上的点 <span class="math inline"><em>x</em></span>，而是考虑沿运动调整射线 <span class="math inline"><em>r</em><sub><em>i</em> → <em>j</em></sub></span> 的点 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub></span>，并将它们视为位于时间 <span class="math inline"><em>j</em></span> 的射线沿线。</p>
<p>具体地，计算了运动调整点 <span class="math inline"><em>x</em><sub><em>i</em> → <em>j</em></sub></span> 后，我们查询 MLP 以预测新轨迹的系数 <span class="math inline">{<em>ϕ</em><sub><em>j</em></sub><sup><em>l</em></sup>(<em>x</em><sub><em>i</em> → <em>j</em></sub>)}<sub><em>l</em> = 1</sub><sup><em>L</em></sup> = GMT(<em>x</em><sub><em>i</em> → <em>j</em></sub>, <em>γ</em>(<em>j</em>))</span>，并使用这些系数计算在时间窗口 <span class="math inline"><em>N</em>(<em>j</em>)</span> 内图像 <span class="math inline"><em>k</em></span> 的相应 3D 点 <span class="math inline">(<em>x</em><sub><em>i</em> → <em>j</em></sub>)<sub><em>j</em> → <em>k</em></sub></span>，使用公式 (2)。这些新的 3D 对应关系随后用于渲染像素颜色，正如在第 3.1 节中描述的“直”射线 <span class="math inline"><em>r</em><sub><em>i</em></sub></span>，只是现在是沿着弯曲的、运动调整射线 <span class="math inline"><em>r</em><sub><em>i</em> → <em>j</em></sub></span>。</p>
<p>即，每个点 <span class="math inline">(<em>x</em><sub><em>i</em> → <em>j</em></sub>)<sub><em>j</em> → <em>k</em></sub></span> 被投影到其源视图 <span class="math inline"><em>I</em><sub><em>k</em></sub></span> 和相机参数 <span class="math inline"><em>P</em><sub><em>k</em></sub></span> 中的特征图 <span class="math inline"><em>F</em><sub><em>k</em></sub></span> 中，以提取 RGB 颜色和图像特征 <span class="math inline"><em>f</em><sub><em>k</em></sub></span>，然后将这些特征聚合并输入到具有时间嵌入 <span class="math inline"><em>γ</em>(<em>j</em>)</span> 的射线变换网络中。结果是沿着 <span class="math inline"><em>r</em><sub><em>i</em> → <em>j</em></sub></span> 在时间 <span class="math inline"><em>j</em></span> 的一系列颜色和密度 <span class="math inline">(<em>c</em><sub><em>j</em></sub>, <em>σ</em><sub><em>j</em></sub>)</span>，这些可以通过体积渲染组合成颜色 <span class="math inline"><em>Ĉ</em><sub><em>j</em> → <em>i</em></sub></span>。</p>
<p>我们可以通过运动遮挡感知 RGB 重建损失来比较 <span class="math inline"><em>Ĉ</em><sub><em>j</em> → <em>i</em></sub>(<em>r</em>)</span> 和目标像素 <span class="math inline"><em>C</em><sub><em>i</em></sub>(<em>r</em>)</span>:</p>
<p><span class="math display">$$
L_ { \text { pho } } = \sum_ {r} \sum_ {j \in N(i)} \hat{W} _ {j \rightarrow i} (r) \rho(C_i(r), \hat {C} _ {j \rightarrow i} (r)).  \tag{3}
$$</span></p>
<p>我们使用广义 Charbonnier 损失 [10] 作为 RGB 损失函数 <span class="math inline"><em>ρ</em></span>。<span class="math inline"><em>Ŵ</em><sub><em>j</em> → <em>i</em></sub>(<em>r</em>)</span> 是通过时间 <span class="math inline"><em>i</em></span> 和 <span class="math inline"><em>j</em></span> 之间的累积 alpha 权重差异计算出的运动遮挡权重，用于解决 NSFF [35] 描述的运动遮挡模糊性（更多细节见补充材料）。请注意，当 <span class="math inline"><em>j</em> = <em>i</em></span> 时，场景没有运动引起的位移，这意味着 <span class="math inline"><em>Ĉ</em><sub><em>j</em> → <em>i</em></sub> = <em>Ĉ</em><sub><em>i</em></sub></span>，没有涉及遮挡权重 (<span class="math inline"><em>Ŵ</em><sub><em>j</em> → <em>i</em></sub> = 1</span>)。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240914165106866.png" /></p>
<blockquote>
<p><strong>图 4. 定性消融实验。</strong></p>
<p>从左到右，我们展示了从我们的系统渲染的新的视图（上）和深度（下）： - <strong>不强制执行时间一致性</strong>。 - <strong>使用场景流场</strong> 而不是运动轨迹来聚合图像特征。 - <strong>使用固定的 DCT 基础</strong> 而不是学习得到的基础来表示运动轨迹。 - <strong>完全配置</strong>。</p>
<p>较简单的配置显著降低了渲染质量，这通过计算移动物体区域的 PSNR 指出。</p>
</blockquote>
<p>我们在图 4 的第一列中展示了强制时间一致性和不强制时间一致性方法的比较。</p>
<h3 id="combining-static-and-dynamic-models">3.3. Combining static and dynamic models</h3>
<p>如 NSFF 所观察到的，使用较小的时间窗口来合成新视图对于静态场景区域的完整性和高质量内容恢复是不够的，因为这些内容可能仅在空间上遥远的帧中可见，尤其是在相机路径不受控的情况下。因此，我们借鉴了 NSFF [35] 的思想，将整个场景建模为两个独立的表示。动态内容 <span class="math inline">(<em>c</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>)</span> 使用时间变化模型进行表示（类似于 NeRF-W [45] 的跨时间瞬态模型）。每个模型的颜色和密度估计也可以单独渲染，得到静态内容的颜色 <span class="math inline"><em>Ĉ</em><sub>st</sub></span> 和动态内容的颜色 <span class="math inline"><em>Ĉ</em><sub>dy</sub></span>。</p>
<p>在结合这两种表示时，我们将光度一致性项的损失公式重写为：</p>
<p><span class="math display">$$
L_ { \text{pho} } = \sum_{r} \sum_{j \in N(i)} \hat{W} _ {j \rightarrow i} ( r ) \rho(C_i( r ), \hat{C} _ {\text{full}, j \rightarrow i} ( r ) ). \tag{4}
$$</span></p>
<p>静态内容 <span class="math inline">(<em>c</em>, <em>σ</em>)</span> 使用时间不变模型进行表示，该模型以与时间变化模型相同的方式进行渲染，但在特征聚合时不进行场景运动调整（即沿视差线）。动态和静态预测结果被合并，并通过结合静态和动态内容的渲染方法生成单一输出颜色 <span class="math inline"><em>Ĉ</em><sub>full, <em>i</em></sub></span>（或 <span class="math inline"><em>Ĉ</em><sub>full, <em>j</em> → <em>i</em></sub></span> 在跨时间渲染过程中）。</p>
<h4 id="image-based-motion-segmentation">Image-based motion segmentation</h4>
<p>基于图像的运动分割。在我们的框架中，我们观察到没有任何初始化的情况下，场景分解往往会被时间不变或时间变化的表示所主导，这种现象在最近的方法中也有观察到 [28, 42]。为了促进分解，Gao 等人 [19] 使用语义分割的掩码来初始化他们的系统，依赖于所有移动物体都可以通过一组候选语义分割标签捕获，并且分割掩码在时间上是准确的。然而，这些假设在许多实际场景中并不成立，正如 Zhang 等人 [79] 所观察到的。</p>
<p>因此，我们提出了一个新的运动分割模块，用于生成分割掩码以监督我们主要的两个组成场景表示。我们的想法受到最近工作 [45, 79] 提出的贝叶斯学习技术的启发，但将其集成到动态视频的体积图像基础渲染 (IBR) 表示中。</p>
<p>在训练我们的主要两个组件场景表示之前，我们联合训练两个轻量级模型以获取每个输入帧 <span class="math inline"><em>I</em><sub><em>i</em></sub></span> 的运动分割掩码 <span class="math inline"><em>M</em><sub><em>i</em></sub></span>。我们使用以下方法建模静态和动态场景内容：</p>
<p>我们使用 IBR-Net [70] 来建模静态场景内容，通过体积渲染沿每条射线聚合来自附近源视图的特征，但不考虑场景运动，来渲染像素颜色 <span class="math inline"><em>B̂</em><sub>st</sub></span>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240914170001551.png" /></p>
<blockquote>
<p><strong>图 5. 运动分割。</strong> 我们展示了完整渲染的 <span class="math inline"><em>B̂</em><sub>full</sub><sup><em>i</em></sup></span>（上图）和与渲染的动态内容 <span class="math inline"><em>α</em><sub>dy</sub><sup><em>i</em></sup> ⋅ <em>B̂</em><sub>dy</sub><sup><em>i</em></sup></span> 叠加的运动分割（下图）。我们的方法能够分割出具有挑战性的动态元素，如移动的阴影、秋千和摇摆的灌木丛。</p>
</blockquote>
<p>我们使用一个 2D 卷积编码器-解码器网络 <span class="math inline"><em>D</em></span> 来建模动态场景内容，该网络从输入帧 <span class="math inline"><em>I</em><sub><em>i</em></sub></span> 中预测 2D 不透明度图 <span class="math inline"><em>α</em><sub>dy</sub><sup><em>i</em></sup></span>、置信度图 <span class="math inline"><em>β</em><sub>dy</sub><sup><em>i</em></sup></span> 和 RGB 图像 <span class="math inline"><em>B̂</em><sub>dy</sub><sup><em>i</em></sup></span>： <span class="math display"><em>B̂</em><sub>dy</sub><sup><em>i</em></sup>, <em>α</em><sub>dy</sub><sup><em>i</em></sup>, <em>β</em><sub>dy</sub><sup><em>i</em></sup> = <em>D</em>(<em>I</em><sub><em>i</em></sub>).</span></p>
<p>完整重建图像从两个模型的输出中逐像素合成： <span class="math display"><em>B̂</em><sub>full</sub><sup><em>i</em></sup>(<em>r</em>) = <em>α</em><sub>dy</sub><sup><em>i</em></sup>(<em>r</em>) ⋅ <em>B̂</em><sub>dy</sub><sup><em>i</em></sup>(<em>r</em>) + (1 − <em>α</em><sub>dy</sub><sup><em>i</em></sup>(<em>r</em>)) ⋅ <em>B̂</em><sub>st</sub>(<em>r</em>).</span> 为了分割移动对象，我们假设观察到的像素颜色在异方差的随机性下是不确定的，并使用具有时间依赖置信度 <span class="math inline"><em>β</em><sub>dy</sub><sup><em>i</em></sup></span> 的柯西分布来建模视频中的观察值。通过取观察值的负对数似然，我们的分割损失写为加权重建损失： <span class="math display">$$
L_{\text{seg} } =
\sum_r \left( \log \beta _ {\text{dy} }^i( r ) + \frac{ \| \hat{B} _ {\text{full} } ^ i( r ) - C_i( r ) \|^2} {\beta _ {\text{dy} } ^ i( r ) } \right).
$$</span></p>
<p>通过使用公式（7）优化这两个模型，我们通过对 <span class="math inline"><em>α</em><sub>dy</sub><sup><em>i</em></sup></span> 进行 0.5 的阈值处理来获得运动分割掩码 <span class="math inline"><em>M</em><sub><em>i</em></sub></span>。我们不需要像 NeRF-W [45] 中那样的 alpha 正则化损失以避免退化，因为我们自然地通过排除网络 <span class="math inline"><em>D</em></span> 的跳跃连接来包含这种归纳基础，这使得 <span class="math inline"><em>D</em></span> 收敛得比静态 IBR 模型更慢。我们在图 5 中展示了我们估计的运动分割掩码叠加在输入图像上的效果。</p>
<h4 id="supervision-with-segmentation-masks">Supervision with segmentation masks</h4>
<p>监督与分割掩码。我们用掩码 <span class="math inline"><strong>M</strong><sub><em>i</em></sub></span> 初始化我们的时间变化模型和时间不变模型，就像在 Omnimatte [43] 中一样，通过对动态区域的时间变化模型渲染结果应用重建损失，对静态区域的时间不变模型渲染结果应用重建损失：</p>
<p><span class="math display"><em>L</em><sub>mask</sub> = ∑<sub><em>r</em></sub>(1 − <strong>M</strong><sub><em>i</em></sub>(<em>r</em>))<em>ρ</em>(<em>Ĉ</em><sub>st</sub>(<em>r</em>), <em>C</em><sub><em>i</em></sub>(<em>r</em>)) + ∑<sub><em>r</em></sub><strong>M</strong><sub><em>i</em></sub>(<em>r</em>)<em>ρ</em>(<em>Ĉ</em><sub>dy</sub><sup><em>i</em></sup>(<em>r</em>), <em>C</em><sub><em>i</em></sub>(<em>r</em>))</span></p>
<p>我们对 <span class="math inline"><strong>M</strong><sub><em>i</em></sub></span> 进行形态学的侵蚀和膨胀操作，以分别获得动态和静态区域的掩码，从而在掩码边界附近关闭损失。我们用 <span class="math inline"><em>L</em><sub>mask</sub></span> 监督系统，并在每 50K 优化步骤后将动态区域的权重衰减一个因子 5。</p>
<h3 id="regularization">3.4. Regularization</h3>
<p><strong>正则化与损失函数</strong>。正如以往的工作所指出的那样，复杂动态场景的单目重建是高度不适定的，仅使用光度一致性不足以避免优化过程中的坏局部极小值 [19, 35]。因此，我们采用了之前工作中使用的正则化方案 [2, 35, 73, 75]，这些方案包括三个主要部分 <span class="math inline"><em>L</em><sub>reg</sub> = <em>L</em><sub>data</sub> + <em>L</em><sub>MT</sub> + <em>L</em><sub>cpt</sub></span>。</p>
<ul>
<li><span class="math inline"><em>L</em><sub>data</sub></span> 是一个数据驱动的项，包括使用 Zhang et al. [80] 和 RAFT [65] 的估计的 `1 单目深度和光流一致性先验。</li>
<li><span class="math inline"><em>L</em><sub>MT</sub></span> 是一个运动轨迹正则化项，鼓励估计的轨迹场保持周期一致性并在空间和时间上平滑。</li>
<li><span class="math inline"><em>L</em><sub>cpt</sub></span> 是一个紧凑性先验，通过熵损失鼓励场景分解为二值，并通过扭曲损失 [2] 减少浮动。</li>
</ul>
<p>有关更多详细信息，请参见补充材料。</p>
<p>总结来说，用于优化我们主要表示以进行时空视图合成的最终组合损失是：</p>
<p><span class="math display"><em>L</em> = <em>L</em><sub>pho</sub> + <em>L</em><sub>mask</sub> + <em>L</em><sub>reg</sub></span></p>
<h2 id="implementation-details">4. Implementation details</h2>
<h4 id="data">data</h4>
<p><strong>数据</strong>。我们在 Nvidia 动态场景数据集 [75] 和 UCSD 动态场景数据集 [37] 上进行了数值评估。每个数据集都包含由同步多视角摄像机记录的 8 个朝前动态场景。我们遵循以往的工作 [35]，从每个序列中提取单目视频，每个视频包含 100 至 250 帧。我们移除了缺少大范围运动物体的帧。评估时，我们采用了以往工作 [35] 中的协议，即使用每个时间点的保留图像进行评估。我们还测试了基于真实场景的单目视频，这些视频具有更具挑战性的摄像机和物体运动 [20]。</p>
<h4 id="view-selection">View selection</h4>
<p><strong>视图选择</strong>。在时变的动态模型中，我们在所有实验中使用帧窗口半径 <span class="math inline"><em>r</em> = 3</span>。对于表示静态场景内容的时不变模型，我们对动态场景基准和真实视频分别采用不同的策略。对于基准测试中，摄像机视点位于离散的摄像机设备位置，我们选择所有在目标时间内时间步长在12帧以内的附近不同视点。对于真实视频，简单地选择最近的源视图可能会由于摄像机基线不足导致重建效果较差。因此，为了确保在渲染的每个像素上，我们有足够的源视图来计算其颜色，我们从更远的帧中选择源视图。如果我们希望为时不变模型选择 <span class="math inline"><em>N</em><sub><em>v</em><em>s</em></sub></span> 个源视图，我们会从输入视频中每隔 <span class="math inline">2<em>r</em><sub>max</sub>/<em>N</em><sub><em>v</em><em>s</em></sub></span> 帧进行子采样以构建候选池，在给定目标时间 <span class="math inline"><em>i</em></span> 时，我们只搜索 <span class="math inline">[<em>i</em> − <em>r</em><sub>max</sub>, <em>i</em> + <em>r</em><sub>max</sub>]</span> 帧内的源视图。我们使用 Li 等人的方法【34】基于 SfM 点的共同可见性和相机相对基线来估计 <span class="math inline"><em>r</em><sub>max</sub></span>。然后，我们通过选择候选池中与目标视点在相机基线距离上最接近的 <span class="math inline"><em>N</em><sub><em>v</em><em>s</em></sub></span> 帧，构建出模型的最终源视图集。我们设定 <span class="math inline"><em>N</em><sub><em>v</em><em>s</em></sub> = 16</span>。</p>
<h4 id="global-spatial-coordinate-embedding"><strong>Global spatial coordinate embedding</strong></h4>
<p><strong>全局空间坐标嵌入</strong>。仅依靠局部图像特征聚合，很难准确确定非表面或被遮挡的表面点的密度，因为不同源视图的特征不一致，正如 NeuRay【39】中所描述的。因此，为了改进密度预测的全局推理，我们在光线变换器的输入中，除了时间嵌入外，还附加了一个全局空间坐标嵌入，类似于【64】中的思想。更多详细信息请参见补充材料。</p>
<h4 id="handling-degeneracy-through-virtual-views"><strong>Handling degeneracy through virtual views</strong></h4>
<p><strong>通过虚拟视图处理退化问题</strong>。先前的工作【35】指出，如果相机和物体的运动大多是共线的，或者场景运动过快以至于无法追踪，优化可能会收敛到不好的局部最小值。受【36】的启发，我们通过使用【80】估计的深度，在每个输入时间点合成八个随机采样的附近视点的图像。在渲染过程中，我们随机采样虚拟视点作为额外的源图像。我们仅将此技术应用于真实场景视频，因为它有助于避免退化解，同时提升渲染质量；而在基准测试中，由于相机和物体的运动是解耦的，正如【20】所述，我们未观察到明显的改进。</p>
<h4 id="time-interpolation"><strong>Time interpolation</strong></h4>
<p><strong>时间插值</strong>。我们的方法还支持通过执行基于场景运动的散点绘制来实现时间插值，这一方法由 NSFF【35】提出。为了在指定的目标分数时间点进行渲染，我们通过聚合来自其对应源视图集的局部图像特征，预测两个相邻输入时间的体积密度和颜色。然后，将预测的颜色和密度通过从我们运动轨迹中导出的场景流进行散点绘制和线性混合，并根据目标分数时间索引进行加权。</p>
<h4 id="setup"><strong>Setup</strong></h4>
<p><strong>设置</strong>。我们使用 COLMAP【57】估计相机姿态。对于每条光线，我们采用 Wang 等人【70】提出的从粗到细的采样策略，每条光线采样 128 次。针对每个场景，我们从头开始训练一个单独的模型，使用 Adam 优化器【29】。两个主要表示的网络架构是 IBRNet【70】架构的变体。我们在欧几里得空间中重建整个场景，没有使用特殊的场景参数化。优化一个 10 秒的视频系统大约需要两天时间，使用 8 台 Nvidia A100 进行渲染时，一个 768 × 432 帧的渲染大约需要 20 秒。有关网络架构、超参数设置和其他详细信息，请参阅补充材料。</p>
<h2 id="evaluation">5. <strong>Evaluation</strong></h2>
<h3 id="baselines-and-error-metrics"><strong>5.1. Baselines and error metrics</strong></h3>
<p>我们将我们的方法与最先进的单视图合成方法进行比较。具体而言，我们与三种最近的基于经典空间的方法进行比较：Nerfies【49】、HyperNeRF【50】，以及两种基于场景流的方法：NSFF【35】和 Gao 等人【19】的动态视图合成（DVS）。为了公平比较，我们使用与我们方法相同的深度、光流和运动分割掩码作为其他方法的输入。</p>
<p>根据先前的工作【35】，我们使用三个标准误差指标来报告每种方法的渲染质量：峰值信噪比（PSNR）、结构相似性（SSIM）和通过 LPIPS【77】的感知相似性，并计算整个场景（Full）和仅限于移动区域（Dynamic Only）的误差。</p>
<h3 id="quantitative-evaluation"><strong>5.2. Quantitative evaluation</strong></h3>
<p>在两个基准数据集上的定量结果见表 1 和表 2。我们的方法在所有误差指标上显著优于先前的最先进方法。特别是，我们的方法在整个场景的 PSNR 上比第二好的方法分别提高了 2dB 和 4dB。在 LPIPS 误差方面，我们的方法也减少了 50%以上，LPIPS 是与真实图像相比的感知质量的主要指标【77】。这些结果表明，我们的框架在恢复高度详细的场景内容方面更为有效。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240918141109083.png" /></p>
<blockquote>
<p>表 1. 在 Nvidia 数据集【75】上的定量评估。</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240918141207923.png" /></p>
<blockquote>
<p>表 2. 在 UCSD 数据集【37】上的定量评估</p>
</blockquote>
<h4 id="ablation-study"><strong>Ablation study</strong></h4>
<p><strong>消融研究。</strong>我们在 Nvidia 动态场景数据集上进行了一项消融研究，以验证我们提出的各种系统组件的有效性。表 3 显示了我们完整系统与各个变体的比较：A) 基线 IBRNet【70】加上额外的时间嵌入；B) 不通过跨时间渲染强制执行时间一致性；C) 使用场景流场在一个时间步内聚合图像特征；D) 在每个样本中预测指向 <span class="math inline">2<em>r</em></span> 附近时间的多个 3D 场景流向量；E) 不使用时间不变的静态场景模型；F) 不通过估计的运动分割掩码使用掩模重建损失；G) 不使用正则化损失。在这项消融研究中，我们对每个模型使用每条光线 64 个样本进行训练。正如表 3 的前三行所示，没有我们的运动轨迹表示和时间一致性，视图合成质量显著下降。整合全局空间坐标嵌入进一步提升了渲染质量。结合静态和动态模型改善了静态元素的质量，这可以从完整场景的指标中看到。最后，去除运动分割或正则化的监督会降低整体渲染质量，展示了提出的损失在优化过程中避免不良局部最小值的价值。</p>
<h3 id="qualitative-evaluation"><strong>5.3. Qualitative evaluation</strong></h3>
<h4 id="dynamic-scenes-dataset"><strong>Dynamic scenes dataset</strong></h4>
<p>动态场景数据集。我们在图 6 和图 7 中对比了我们的方法与三种先前的最先进方法【19, 35, 50】在两个数据集测试视图上的定性结果。之前的动态 NeRF 方法在渲染移动物体的细节时存在困难，如气球、人的脸部和衣物的纹理过度模糊。相比之下，我们的方法合成了静态和动态场景内容的照片级真实新视图，这些视图最接近真实图像。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240918140731099.png" /></p>
<blockquote>
<p>图 6. 在 Nvidia 数据集【75】上的定性比较。</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240918140755719.png" /></p>
<blockquote>
<p>图 7. 在 UCSD 数据集【37】上的定性比较。</p>
</blockquote>
<h4 id="in-the-wild-videos"><strong>In-the-wild videos</strong></h4>
<p>真实场景视频。我们展示了对复杂动态场景的真实视频的定性比较。图 8 展示了与基于动态 NeRF 方法的比较，图 9 展示了与使用深度信息【80】的点云渲染的比较。我们的方法合成了照片级真实的新视图，而先前的动态 NeRF 方法在恢复静态和动态场景内容的高质量细节方面存在困难，如图 8 中的衬衫皱褶和狗的毛发。另一方面，显式深度变形在靠近遮挡区域和视野之外的区域会产生孔洞。更多比较请参见补充视频。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240918141427762.png" /></p>
<blockquote>
<p>图 8. 在真实场景视频上的定性比较。我们展示了复杂动态场景的 10 秒视频的结果。最左侧列展示了每个视频的开始和结束帧；右侧展示了从我们的方法和先前最先进的方法【19, 35, 50】渲染的中间时间的新视图。</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240918141503143.png" /></p>
<blockquote>
<p>图 9. 从左到右，我们展示了输入图像以及通过 Zhang 等人【80】的显式深度变形和我们的方法渲染的对应新视图。</p>
</blockquote>
<h2 id="discussion-and-conclusion">6. <strong>Discussion and conclusion</strong></h2>
<h4 id="limitations"><strong>Limitations</strong></h4>
<p><strong>限制</strong>。与专为静态或准静态场景设计的方法相比，我们的方法在视点变化方面有限；由于初始深度和光流估计不准确，我们的方法无法处理小而快速移动的物体（图 10 左）。此外，与先前的动态 NeRF 方法相比，合成的视图不完全多视角一致，静态内容的渲染质量取决于选择的源视图（图 10 中）。我们的方法也对来自真实视频的退化运动模式敏感，其中物体和相机运动大多是共线的，但我们在补充材料中展示了处理这些情况的启发式方法。此外，我们的方法只能合成仅出现在远距离时间的新动态内容（图 10 右）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ffxxfan/pictures@main/img/image-20240918141657805.png" /></p>
<blockquote>
<p>图 10. 局限性。我们的方法可能无法建模移动的细长物体，例如移动的牵引绳（左）。我们的方法可能无法渲染仅在远距离帧中可见的动态内容（中）。如果为给定像素聚合的源视图特征不足，则渲染的静态内容可能不真实或为空白（右）。</p>
</blockquote>
<h4 id="conclusion"><strong>Conclusion</strong></h4>
<p><strong>结论</strong>。我们提出了一种新的空间-时间视图合成方法，适用于描绘复杂动态场景的单目视频。通过在体积 IBR 框架中表示动态场景，我们的方法克服了最近方法无法处理复杂相机和物体运动的长视频的局限性。我们已展示了我们的方法能够从真实场景动态视频中合成照片级真实的新视图，并在动态场景基准测试中取得了显著的改进，相比于先前的最先进方法。</p>
<h4 id="acknowledgements"><strong>Acknowledgements</strong></h4>
<p><strong>致谢</strong>。感谢 Andrew Liu、Richard Bowen 和 Lucy Chai 的富有成效的讨论，感谢 Rick Szeliski 和 Ricardo Martin-Brualla 的有益校对。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://ffxxfan.github.io/">华生</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://ffxxfan.github.io/2024/09/18/DynIBaR%20Neural%20Dynamic%20Image-Based%20Rendering%20%E7%BF%BB%E8%AF%91/">https://ffxxfan.github.io/2024/09/18/DynIBaR Neural Dynamic Image-Based Rendering 翻译/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://ffxxfan.github.io" target="_blank">华生</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a><a class="post-meta__tags" href="/tags/%E6%96%B0%E8%A7%86%E8%A7%92%E5%90%88%E6%88%90/">新视角合成</a><a class="post-meta__tags" href="/tags/%E5%8D%95%E7%9B%AE%E9%95%BF%E8%A7%86%E9%A2%91%E6%96%B0%E8%A7%86%E8%A7%92%E5%90%88%E6%88%90/">单目长视频新视角合成</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423153723711.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/04/23/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%A0%87%E6%B3%A8%E6%96%B9%E6%B3%95/" title="文本数据标注工具"><img class="cover" src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250423170608150.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">文本数据标注工具</div></div><div class="info-2"><div class="info-item-1">文本数据标注的工具的安装使用，以及部分报错，目前只更新了 doccano。</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/04/24/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attention%20Control%20%E7%BF%BB%E8%AF%91/" title="Prompt-to-Prompt Image Editing with Cross Attention Control"><img class="cover" src="https://raw.githubusercontent.com/ffxxfan/pictures/main/20250424002826556.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-24</div><div class="info-item-2">Prompt-to-Prompt Image Editing with Cross Attention Control</div></div><div class="info-2"><div class="info-item-1">论文翻译</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#dynibar-neural-dynamic-image-based-rendering"><span class="toc-text">DynIBaR: Neural Dynamic Image-Based Rendering</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related-work"><span class="toc-text">2. Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B0%E8%A7%86%E8%A7%92%E5%90%88%E6%88%90"><span class="toc-text">新视角合成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E5%9C%BA%E6%99%AF%E6%96%B0%E8%A7%86%E8%A7%92%E5%90%88%E6%88%90"><span class="toc-text">动态场景新视角合成</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dynamic-image-based-rendering"><span class="toc-text">3. Dynamic Image-Based Rendering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#motion-adjusted-feature-aggregation"><span class="toc-text">3.1 Motion-adjusted feature aggregation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cross-time-rendering-for-temporal-consistency"><span class="toc-text">3.2. Cross-time rendering for temporal consistency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#combining-static-and-dynamic-models"><span class="toc-text">3.3. Combining static and dynamic models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#image-based-motion-segmentation"><span class="toc-text">Image-based motion segmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#supervision-with-segmentation-masks"><span class="toc-text">Supervision with segmentation masks</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#regularization"><span class="toc-text">3.4. Regularization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#implementation-details"><span class="toc-text">4. Implementation details</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#data"><span class="toc-text">data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#view-selection"><span class="toc-text">View selection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#global-spatial-coordinate-embedding"><span class="toc-text">Global spatial coordinate embedding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#handling-degeneracy-through-virtual-views"><span class="toc-text">Handling degeneracy through virtual views</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#time-interpolation"><span class="toc-text">Time interpolation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#setup"><span class="toc-text">Setup</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#evaluation"><span class="toc-text">5. Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#baselines-and-error-metrics"><span class="toc-text">5.1. Baselines and error metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#quantitative-evaluation"><span class="toc-text">5.2. Quantitative evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ablation-study"><span class="toc-text">Ablation study</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#qualitative-evaluation"><span class="toc-text">5.3. Qualitative evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dynamic-scenes-dataset"><span class="toc-text">Dynamic scenes dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#in-the-wild-videos"><span class="toc-text">In-the-wild videos</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discussion-and-conclusion"><span class="toc-text">6. Discussion and conclusion</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#limitations"><span class="toc-text">Limitations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#conclusion"><span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#acknowledgements"><span class="toc-text">Acknowledgements</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By 华生</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">一起学会闪闪发光吧！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.3.5"></script><script src="/js/main.js?v=5.3.5"></script><script src="/js/tw_cn.js?v=5.3.5"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>